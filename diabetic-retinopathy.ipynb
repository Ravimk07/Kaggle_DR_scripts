{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, random, sys, cv2, matplotlib, csv, keras\nfrom subprocess import check_output\nfrom datetime import datetime\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.optimizers import SGD, Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\nfrom keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"NUM_CLASSES = 5\n\n# we need images of same size so we convert them into the size\nWIDTH = 128\nHEIGHT = 128\nDEPTH = 3\ninputShape = (HEIGHT, WIDTH, DEPTH)\n\n# initialize number of epochs to train for, initial learning rate and batch size\nEPOCHS = 15\nINIT_LR = 1e-3\nBS = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96671ff26517c1b2df1c41f058737cf56ee4cd4e"},"cell_type":"code","source":"print(\"Loading images at...\"+ str(datetime.now()))\nsys.stdout.flush()\n\nImageNameDataHash = {}\nimages = os.listdir(\"/kaggle/working/../input/\")\n\nfor imageFileName in images:\n    if(imageFileName == \"trainLabels.csv\"):\n        continue\n    img = load_img(os.path.join(os.path.sep, \"/kaggle/working/../input/\", imageFileName))\n    arr = img_to_array(img)\n    dim1 = arr.shape[0]\n    dim2 = arr.shape[1]\n    dim3 = arr.shape[2]\n    if (dim1 < HEIGHT or dim2 < WIDTH or dim3 < DEPTH):\n        print(\"Error image dimensions are less than expected \"+str(arr.shape))\n    \n    arr = cv2.resize(arr, (HEIGHT,WIDTH))\n    \n    dim1 = arr.shape[0]\n    dim2 = arr.shape[1]\n    dim3 = arr.shape[2]\n    \n    if (dim1 != HEIGHT or dim2 != WIDTH or dim3 != DEPTH):\n        print(\"Error after resize, image dimensions are not equal to expected \"+str(arr.shape))\n    \n    arr = np.array(arr, dtype=\"float\") / 255.0\n    imageFileName = imageFileName.replace('.jpeg','')\n    \n    ImageNameDataHash[str(imageFileName)] = np.array(arr) \n        \nprint(\"Loaded \" + str(len(ImageNameDataHash)) + \" images at...\"+ str(datetime.now())) # 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fa9eb6c2bbc24bd9bcb6947f7a4049995b57bbb"},"cell_type":"code","source":"random.seed(10)\nprint(\"Reading trainLabels.csv...\")\ndf = pd.read_csv('/kaggle/working/../input/trainLabels.csv', sep=',')\nprint(type(df))\n\nrow_count = df.shape[0]\ncol_count = df.shape[1]\nprint(\"row_count=\"+str(row_count)+\" col count=\"+str(col_count))\n\ndf[\"PatientID\"] = ''\nheader_list = list(df.columns)\nprint(header_list)\n\nImageLevelHash = {}\npatientIDList = []\nuniquePatientIDList = []\n\nfor index, row in df.iterrows():\n    key = row[0] + ''\n    patientID = row[0] + ''\n    patientID = patientID.replace('_right', '')\n    patientID = patientID.replace('_left', '')\n    df.at[index, 'PatientID'] = patientID\n    patientIDList.append(patientID)\n    ImageLevelHash[key] = str(row[1])\n    \nuniquePatientIDList = sorted(set(patientIDList))\ncount = 0\n\nfor patientID in uniquePatientIDList:\n    left_level = ImageLevelHash[str(patientID + '_left')]\n    right_level = ImageLevelHash[str(patientID + '_right')]\n    \n    if(left_level != right_level):\n        count = count + 1\n\nprint(\"count of images with both left and right eye level not matching=\"+str(count))\nprint(\"number of unique patients=\"+str(len(uniquePatientIDList)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25a220e816bc9a03c896d640d24201f0cd5b78d5"},"cell_type":"code","source":"imageNameArr = []\ndataArr = []\n\nkeepImages =  list(ImageNameDataHash.keys())\ndf = df[df['image'].isin(keepImages)]\nfor index, row in df.iterrows():\n    key = str(row[0])\n    if key in ImageNameDataHash:\n        imageNameArr.append(key)\n        dataArr.append(np.array(ImageNameDataHash[key]))\n        \ndf2 = pd.DataFrame({'image': imageNameArr, 'data': dataArr})\ndf2_header_list = list(df2.columns)\n\nif len(df) != len(df2):\n    print(\"Error length of df != df2\")\nfor idx in range(0, len(df)):\n    if (df.loc[df.index[idx], 'image'] != df2.loc[df2.index[idx], 'image']):\n        print(\"Error \" + df.loc[df.index[idx], 'image'] + \"==\" + df2.loc[df2.index[idx], 'image'])\n        \ndf = pd.merge(df2, df, left_on='image', right_on='image', how='outer')\ndf_header_list = list(df.columns) \nprint(df_header_list) # 'image', 'data', level', 'PatientID'\nprint(len(df)) # 1000\nprint(df.sample())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d95eb147e1017fffec2efdb150a5b127ba3e3d9"},"cell_type":"code","source":"sample = df.loc[df.index[0], 'data']\nprint(\"Sample Image\")\nprint(type(sample)) # <class 'numpy.ndarray'>\nprint(sample.shape) # 128,128,3\nfrom matplotlib import pyplot as plt\nplt.imshow(sample, interpolation='nearest')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4334b161b52ee7b9c20d23e01ca340ed158430c"},"cell_type":"code","source":"X = df['data']\nY = to_categorical(np.array(df['level']), num_classes=NUM_CLASSES)\nprint(\"Partition of image into 60:20:20\")\nsys.stdout.flush()\nunique_ids = df.PatientID.unique()\nprint('unique_ids shape='+ str(len(unique_ids))) #500\n\ntrain_ids, not_train_ids = train_test_split(unique_ids, test_size = 0.40, random_state = 10)\nvalid_ids, test_ids = train_test_split(not_train_ids, test_size = 0.50, random_state = 10)\n\ntrainid_list = train_ids.tolist()\nvalidid_list = valid_ids.tolist()\ntestid_list = test_ids.tolist()\n\ntraindf = df[df.PatientID.isin(trainid_list)]\nvalSet = df[df.PatientID.isin(validid_list)]\ntestSet = df[df.PatientID.isin(testid_list)]\n\ntraindf = traindf.reset_index(drop=True)\nvalSet = valSet.reset_index(drop=True)\ntestSet = testSet.reset_index(drop=True)\nprint(traindf.head())\nprint(valSet.head())\nprint(testSet.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b7a47653b31f8056ab6f90da4bdaf5ee759d173"},"cell_type":"code","source":"trainX = traindf['data']\ntrainY = traindf['level']\n\nvalX = valSet['data']\nvalY = valSet['level']\n\ntestX = testSet['data']\ntestY = testSet['level']\n\nprint('trainX shape=', trainX.shape[0], 'valX shape=', valX.shape[0], 'testX shape=', testX.shape[0]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87900a98e59010f5bb6a632ea3e379ee8fe696ff"},"cell_type":"code","source":"trainY =  to_categorical(trainY, num_classes=NUM_CLASSES)\nvalY =  to_categorical(valY, num_classes=NUM_CLASSES)\ntestY =  to_categorical(testY, num_classes=NUM_CLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c39fcb8ab9611e2f331bd2d08f14db22b73f00e"},"cell_type":"code","source":"from numpy import zeros\n\nXtrain = np.zeros([trainX.shape[0],HEIGHT, WIDTH, DEPTH])\nfor i in range(trainX.shape[0]): # 0 to traindf Size -1\n    Xtrain[i] = trainX[i]\nXval = np.zeros([valX.shape[0],HEIGHT, WIDTH, DEPTH])\nfor i in range(valX.shape[0]): # 0 to traindf Size -1\n    Xval[i] = valX[i]\nXtest = np.zeros([testX.shape[0],HEIGHT, WIDTH, DEPTH])\nfor i in range(testX.shape[0]): # 0 to traindf Size -1\n    Xtest[i] = testX[i]\n\nprint(Xtrain.shape) # (750,128,128,3)\nprint(Xval.shape) # (250,128,128,3)\nprint(Xtest.shape) # (750,128,128,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7822a27aa16a05f7552e38d9fb629ffc76f4d8d7"},"cell_type":"code","source":"\"\"\"\nimport theano\nprint(theano.config.device)\nprint(theano.config.floatX)\n\"\"\"\n\n\n\n\"\"\"\nimport scipy.misc\nfor img in enumerate(Xtrain):\n    if not os.path.exists('../input/training'):\n        os.makedirs('../input/training')\n    scipy.misc.toimage(img, cmin=0.0, cmax=...).save('../input/training/' + str(df.loc[df.data[img], 'image']) + '.jpeg')\nfor img in enumerate(Xval):\n    if not os.path.exists('../input/validation'):\n        os.makedirs('../input/validation')\n    scipy.misc.toimage(img, cmin=0.0, cmax=...).save('../input/validation/' + str(df.loc[df.data[img], 'image']) + '.jpeg')\nfor img in enumerate(Xtest):\n    if not os.path.exists('../input/test'):\n        os.makedirs('../input/test')\n    scipy.misc.toimage(img, cmin=0.0, cmax=...).save('../input/test/' + str(df.loc[df.data[img], 'image']) + '.jpeg')\n\"\"\"\n\n\n\n\"\"\"\n# Aquire training, validation & prediction sets\nfrom keras.preprocessing import image\nbatches = image.ImageDataGenerator().flow_from_directory(\n    '../input/training',\n    target_size=(512, 512), class_mode='categorical', shuffle=True, batch_size=16\n)\n\nval_batches = image.ImageDataGenerator().flow_from_directory(\n    '../input/validation',\n    target_size=(512, 512), class_mode='categorical', shuffle=True, batch_size=16\n)\n\ntest_batches = image.ImageDataGenerator().flow_from_directory(\n    '../input/test',\n    target_size=(512, 512), class_mode=None, shuffle=False, batch_size=32\n)\n\"\"\"\n\n\n\"\"\"\nmodel = Sequential()\n# Increase the input shape and generator shape for more features. Number of features = side * side (224 * 224)\nmodel.add(Conv2D(16, (3, 3), activation='relu', input_shape=(512, 512, 3)))\nmodel.add(Conv2D(16, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.2))\n\"\"\"\n\n\"\"\"\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.2))\n\"\"\"\n\n\"\"\"\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.2))\n\"\"\"\n\n\"\"\"\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.2))\n\"\"\"\n\n\"\"\"\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.2))\n\"\"\"\n\n\"\"\"\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.2))\n\"\"\"\n\n\"\"\"\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(5, activation='softmax'))\n\"\"\"\n\n\"\"\"\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\"\"\"\n\n\"\"\"\nmodel.fit_generator(batches, validation_data=val_batches, shuffle=True, epochs=10)\n\"\"\"\n\n\"\"\"\npreds = model.predict_generator(test_batches, test_batches.samples)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adc7449fe9de9eda986d57c3a392cc4b52052f3c"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=inputShape))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17254d372f9b0e5a28a38ef7de082c9942b37452"},"cell_type":"code","source":"model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d70a7e781752ae8a430b10bf4970a4bdb493eb19"},"cell_type":"code","source":"model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1073067e1d7139432f6a190c383624ad85eed6e8"},"cell_type":"code","source":"model.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(output_dim=NUM_CLASSES, activation='softmax')) \nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b80744bbc10b302b06ef2244e4ded1fd4bc92082"},"cell_type":"code","source":"print(\"compiling model...\")\nsys.stdout.flush()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f07654eec1f38e578f17261564dda96efe47bf"},"cell_type":"code","source":"#summary\nfrom keras.utils import print_summary\nprint_summary(model, line_length=None, positions=None, print_fn=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"815c0bfa9af7fcf48b9c2a45e2626e6c13a4b097"},"cell_type":"code","source":"#construct the image generator for data augmentation\nprint(\"Generating images...\")\nsys.stdout.flush()\naug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\")\n\nprint(\"training network...\")\nsys.stdout.flush()\n\nH = model.fit_generator(aug.flow(Xtrain, trainY, batch_size=BS), validation_data=(Xval, valY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"feba1c09d5c1c702997ed9676e8b369382229b1a"},"cell_type":"code","source":"def letsPredict(predictit):\n    array = [[0 for x in range(5)] for y in range(len(predictit))] \n    for i, value in enumerate(predictit):\n        if max(value[0], value[1], value[2], value[3], value[4]) == value[0]: \n            array[i] = [1., 0., 0., 0., 0.]\n        elif max(value[0], value[1], value[2], value[3], value[4]) == value[1]: \n            array[i] = [0., 1., 0., 0., 0.]\n        elif max(value[0], value[1], value[2], value[3], value[4]) == value[2]: \n            array[i] = [0., 0., 1., 0., 0.]\n        elif max(value[0], value[1], value[2], value[3], value[4]) == value[3]: \n            array[i] = [0., 0., 0., 1., 0.]\n        elif max(value[0], value[1], value[2], value[3], value[4]) == value[4]: \n            array[i] = [0., 0., 0., 0., 1.]\n        else:\n            array[i] = [1., 0., 0., 0., 0.]\n    return array\n\npredict = model.predict(Xtest, batch_size=BS, verbose = 1, steps = None)\nprint(predict)\nXtest1 = letsPredict(predict)\nprint(Xtest1)   \nprint(testY)    \n\nevaluate = model.evaluate(Xtest1, testY, verbose = 1, steps = None)\nprint(evaluate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87fdf66db4cfc41a285bf2bdb97d1b7b281ad79a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4076d2b43432751a4f4cdb3a51b414b2d06c3206"},"cell_type":"code","source":"\"\"\"\nimport cv2\nimport datetime as dt\nimport h5py\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as plb\nimport numpy as np\nimport os\nimport pandas as pd\nfrom glob import glob\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8535f319cac54a3f6d018588bae2a8b0f0589470"},"cell_type":"code","source":"\"\"\"\ndef proc_images():\n    start = dt.datetime.now()\n    # ../input/\n    PATH = os.path.abspath(os.path.join('..', 'input'))\n    # ../input/sample/images/\n    ##SOURCE_IMAGES = os.path.join(PATH) #os.listdir(\"/kaggle/working/../input/\") \n    # ../input/sample/images/*.png\n    images = glob(os.path.join(PATH, \"*.jpeg\"))\n    # Load labels\n    labels = pd.read_csv('../input/trainLabels.csv')\n       \n    # Set the disease type you want to look for\n    #disease=\"Infiltration\"\n    \n    # Size of data\n    NUM_IMAGES = len(images)\n    HEIGHT = 256\n    WIDTH = 256\n    CHANNELS = 3\n    SHAPE = (HEIGHT, WIDTH, CHANNELS)\n    \n    with h5py.File('resized.h5', 'w') as hf: \n        for i,img in enumerate(images):            \n            # Images\n            image = cv2.imread(img)\n            image = cv2.resize(image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC)\n            Xset = hf.create_dataset(\n                name='X'+str(i),\n                data=image,\n                shape=(HEIGHT, WIDTH, CHANNELS),\n                maxshape=(HEIGHT, WIDTH, CHANNELS),\n                compression=\"gzip\",\n                compression_opts=9)\n            # Labels\n            base = os.path.basename(img)\n            finding = labels.loc[labels[\"image\"] == base, \"level\"] #labels[\"level\"][labels[\"image\"] == base].values[0] #[lvl for ig, lvl in labels.values if ig == base]  \n            print(finding)\n            yset = hf.create_dataset(\n                name='y'+str(i),\n                shape=(1,),\n                maxshape=(None,),\n                compression=\"gzip\",\n                compression_opts=9)\n         #   if 1 in finding:\n         #       finding = 1\n         #       yset = finding\n         #   else:\n         #      finding = 0\n         #       yset = finding\n            yset = finding\n            end=dt.datetime.now()\n            print(\"\\r\", i, \": \", (end-start).seconds, \"seconds\", end=\"\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef7db0d8e98c91137cd681d0f04840d4dd382bb7"},"cell_type":"code","source":"\"\"\"\nproc_images()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cee03dff76317227f3d3426ad903da43b4a672cf"},"cell_type":"code","source":"\"\"\"\nwith h5py.File('resized.h5', 'r') as hf:\n    plb.imshow(hf[\"X56\"])\n    print(hf[\"y56\"].value)\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}